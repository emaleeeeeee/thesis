{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Notebook: Theme extraction\n\n<center><img src=\"https://media-exp1.licdn.com/dms/image/C5112AQHxkUuGyjk_Zw/article-cover_image-shrink_600_2000/0/1520139427384?e=1635379200&v=beta&t=E1kiu2v6CzpQ0OnIAhniRoEeS0zwlZ-PeA_-Au1_ang\" title=\"Python Logo\"/></center>\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00000-69b056be-bdf0-4d75-aedf-704db13fd827",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "The objective of this notebook is to give an example of unsupervised thematic extraction on simple and complex domain.\n\n**Topic analysis** (also called topic detection, topic modeling, or topic extraction) is a machine learning technique that organizes and understands large collections of text data, by assigning “tags” or categories according to each individual text’s topic or theme.\n\nTopic analysis uses natural language processing (NLP) to break down human language so that you can find patterns and unlock semantic structures within texts to extract insights and help make data-driven decisions.\n\nThe two most common approaches for topic analysis with machine learning are NLP topic modeling and NLP topic classification.\n\n**Topic modeling** is an unsupervised machine learning technique. This means it can infer patterns and cluster similar expressions without needing to define topic tags or train data beforehand. This type of algorithm can be applied quickly and easily, but there’s a downside – they are rather inaccurate.\n\n**Text classification** or **topic extraction from text**, on the other hand, needs to know the topics of a text before starting the analysis, because you need to tag data in order to train a topic classifier. Although there’s an extra step involved, topic classifiers pay off in the long run, and they’re much more precise than clustering techniques.",
   "metadata": {
    "tags": [],
    "cell_id": "00001-7470a1ba-f64c-458f-b3f5-5093309d6881",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# 1. Install libraries",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00003-bff9dabd-16a2-404e-a8f3-78c101d95dda",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "To use our coding example, we will use the libraries :\n- **nltk**: state-of-the art library for NLP analysis (https://github.com/nltk/nltk)\n- **stanza**: famous NLP library made by the university of Stanford for NLP analysis (https://github.com/stanfordnlp/stanza)\n- **rake_nltk**: a Python implementation of the RAKE algorithm using the NLTK library (https://github.com/csurfer/rake-nltk)\n- **networkx** (https://github.com/networkx)",
   "metadata": {
    "tags": [],
    "cell_id": "00003-38756185-182c-48aa-a0fb-53721d4fb071",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 1.1 Install the package from pip",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00004-06c12906-1202-4c43-bed9-338729a538dc",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "We first install the different libraries",
   "metadata": {
    "tags": [],
    "cell_id": "00004-1d75e19a-b334-40a7-9f67-c46e4c027723",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-447c95b1-b0aa-471f-8566-31f8f2979062",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9c29af0c",
    "execution_start": 1633233171659,
    "execution_millis": 3744,
    "is_output_hidden": true,
    "deepnote_cell_type": "code"
   },
   "source": "# Install the nltk package\n!pip install nltk",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: nltk in /shared-libs/python3.7/py/lib/python3.7/site-packages (3.6.3)\nRequirement already satisfied: click in /shared-libs/python3.7/py/lib/python3.7/site-packages (from nltk) (8.0.1)\nRequirement already satisfied: joblib in /shared-libs/python3.7/py/lib/python3.7/site-packages (from nltk) (1.0.1)\nRequirement already satisfied: tqdm in /shared-libs/python3.7/py/lib/python3.7/site-packages (from nltk) (4.62.3)\nRequirement already satisfied: regex in /shared-libs/python3.7/py/lib/python3.7/site-packages (from nltk) (2021.8.28)\nRequirement already satisfied: importlib-metadata in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from click->nltk) (4.8.1)\nRequirement already satisfied: zipp>=0.5 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.5.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.10.0.2)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00010-c5e94d20-e1bb-45c4-8d56-4bd178861e94",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "edb8d055",
    "execution_start": 1633233175419,
    "execution_millis": 3485,
    "is_output_hidden": true,
    "deepnote_cell_type": "code"
   },
   "source": "# Install the rake_nltk package\n!pip install rake_nltk",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: rake_nltk in /root/venv/lib/python3.7/site-packages (1.0.6)\nRequirement already satisfied: nltk<4.0.0,>=3.6.2 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from rake_nltk) (3.6.3)\nRequirement already satisfied: joblib in /shared-libs/python3.7/py/lib/python3.7/site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (1.0.1)\nRequirement already satisfied: regex in /shared-libs/python3.7/py/lib/python3.7/site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (2021.8.28)\nRequirement already satisfied: click in /shared-libs/python3.7/py/lib/python3.7/site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (8.0.1)\nRequirement already satisfied: tqdm in /shared-libs/python3.7/py/lib/python3.7/site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (4.62.3)\nRequirement already satisfied: importlib-metadata in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from click->nltk<4.0.0,>=3.6.2->rake_nltk) (4.8.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from importlib-metadata->click->nltk<4.0.0,>=3.6.2->rake_nltk) (3.10.0.2)\nRequirement already satisfied: zipp>=0.5 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from importlib-metadata->click->nltk<4.0.0,>=3.6.2->rake_nltk) (3.5.0)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00002-b6d57d9a-4a4e-444c-8b14-54f3890ff937",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8edb8cdd",
    "execution_start": 1633233178909,
    "execution_millis": 3401,
    "is_output_hidden": true,
    "deepnote_cell_type": "code"
   },
   "source": "# Install the networkx package\n!pip install networkx",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: networkx in /root/venv/lib/python3.7/site-packages (2.6.3)\r\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00010-0c13d181-55b5-4118-a385-22a7cd3a5572",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f291eaeb",
    "execution_start": 1633233182318,
    "execution_millis": 3457,
    "is_output_hidden": true,
    "deepnote_cell_type": "code"
   },
   "source": "!pip install tqdm",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: tqdm in /shared-libs/python3.7/py/lib/python3.7/site-packages (4.62.3)\r\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 1.2 Download the plugins within the libraries",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00009-baa8dee1-c5f3-43d3-a6d4-d292d2c9130d",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Stanza package requires to download the material of the language that will be used inside our code. In our case, our example are English based document. We will download the English package by usin the command `stanza.download(<language>)` \n\nMore information on how to get started with the package can be found at: https://stanfordnlp.github.io/stanza/#getting-started\n\nNote: Stanza has a strange bug and its installation has been put into the init.ipbyb",
   "metadata": {
    "tags": [],
    "cell_id": "00010-3287a33b-aa48-4e58-ac99-8cb3d19ec197",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00014-41a5234b-b501-40da-b2b5-3802a0da31dc",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7152ec09",
    "execution_start": 1633233277743,
    "execution_millis": 71,
    "is_output_hidden": false,
    "deepnote_cell_type": "code"
   },
   "source": "import nltk\n\nnltk.download('stopwords')\nnltk.download('punkt')",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 10,
     "data": {
      "text/plain": "True"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 2. RAKE Algorithm",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00013-165b88fb-7bbc-4c18-a0aa-76e5145c07c1",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 2.1 Presentation",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00017-df155ed4-3ac9-4318-b58e-65a0d2938f84",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Rake also known as **Rapid Automatic Keyword Extraction** is a keyword extraction algorithm that is extremely efficient which operates on individual documents to enable an application to the dynamic collection, it can also be applied on the new domains very easily and also very effective in handling multiple types of documents, especially the type of text which follows specific grammar conventions.\nRake is based on the observations that keywords frequently contain multiple words with standard punctuation or stop words or we can say functioning words like ‘and’, ‘of’, ‘the’, etc with minimum lexical meaning. Stop words are typically dropped within all the informational systems and also not included in various text analyses as they are considered to be meaningless. Words that are considered to carry a meaning related to the text are described as the content bearing and are called as content words.\n\nSource: https://medium.datadriveninvestor.com/rake-rapid-automatic-keyword-extraction-algorithm-f4ec17b2886c",
   "metadata": {
    "tags": [],
    "cell_id": "00014-c937c019-dfb7-4516-9bb1-1578481929d5",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<center><img src=\"https://miro.medium.com/max/1276/0*HlO7We9cJhaLa5QT\" title=\"Python Logo\"/></center>\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00019-17c1d0a6-e785-4548-b0c5-69b6cfc50a76",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 2.2 Let's test the RAKE algorithm on a simple example",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00019-7f42bba2-68d3-4691-ab25-0ce6ed95d9ac",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Let's now analyze the RAKE algorithm on a simple example. We have a text: `this film is great but the movie was awful. The theatre was amazing`. The objective of our work is to extract the different subjects. In this case, the answers we are waiting are:\n- `great film` \n- `movie awful` \n- `theatre amazing`",
   "metadata": {
    "tags": [],
    "cell_id": "00015-9ad89301-a6d1-4164-8300-f1fa01c4f6ff",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00015-97532396-fb01-4496-8773-1255ecc3301e",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7bf2d0c5",
    "execution_start": 1633233187790,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "# Text Example\ntxt = \"this film is great but the movie was awful. The theatre was amazing\"",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Define the stopwords list:",
   "metadata": {
    "tags": [],
    "cell_id": "00019-b78350ea-e28d-40c2-9d68-068b88cebc13",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00030-00787afb-8c13-477b-82d9-73913eefb15e",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "c3f0066f",
    "allow_embed": false,
    "execution_start": 1633233187799,
    "execution_millis": 1,
    "is_output_hidden": true,
    "deepnote_cell_type": "code"
   },
   "source": "from nltk.corpus import stopwords\n\nstopword = list(stopwords.words('english'))\nstopword.remove('not')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Here is below a code to use the RAKE Algorithm:",
   "metadata": {
    "tags": [],
    "cell_id": "00017-47e253a3-4c2d-49f7-9da5-71f26f840478",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00021-4c7c1318-b6f1-466e-ad26-0d37aeae512d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f6f5d03f",
    "execution_start": 1633233285388,
    "execution_millis": 14,
    "deepnote_cell_type": "code"
   },
   "source": "# Import the libraries\nfrom rake_nltk import Rake\nfrom nltk.corpus import stopwords \n\n# Define the object RAKE\nr = Rake(stopwords = stopword) # Uses stopwords for english from NLTK, and all puntuation characters.Please note that \"hello\" is not included in the list of stopwords.\n\n# Make the extraction\na=r.extract_keywords_from_text(txt)\nb=r.get_ranked_phrases()\nc=r.get_ranked_phrases_with_scores()\n\nprint(\"Original text:\", txt)\nprint(\"Extracted Keywords from the text:\", a)\nprint(\"Ranked phrases:\", b)\nprint(\"Ranked phrases with scores:\", c)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Original text: this film is great but the movie was awful. The theatre was amazing\nExtracted Keywords from the text: None\nRanked phrases: ['theatre', 'movie', 'great', 'film', 'awful', 'amazing']\nRanked phrases with scores: [(1.0, 'theatre'), (1.0, 'movie'), (1.0, 'great'), (1.0, 'film'), (1.0, 'awful'), (1.0, 'amazing')]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 2.3 Limits of the RAKE Algorithm",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00026-5efa0c72-b592-4459-a1fb-5eb08b7a951d",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "What we can observe is that the algorithm succeeded in extracting the different important word but did not succeeded in associating the words that are of the same concept. To improve the method, we need another method to categorize first the different part of a comment, then use the RAKE algorithm to analyze the subject",
   "metadata": {
    "tags": [],
    "cell_id": "00018-3029f9ec-311e-4fb2-a230-3d99dd395279",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# 3. Package to separate the different thematic in a comment",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00008-706e3d60-441b-4840-a898-fa62195099c8",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 3.1 Phrase separation based on the ASBA method",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00030-fc6143b8-c535-487e-8937-8a8de034e027",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "One challenge when you want to use the RAKE algorithm is to separate first the different concept. For this, we can inspirate ourselves from the ASBA method, presented in this article: https://medium.com/analytics-vidhya/aspect-based-sentiment-analysis-a-practical-approach-8f51029bbc4a\n\nWe upgrade this code so that it can take into account of our business case",
   "metadata": {
    "tags": [],
    "cell_id": "00014-c920cf0c-ceff-4d10-b602-a68ad610550f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-e52d6300-a8d7-485b-b30c-aedbf3fce802",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8eb1c49e",
    "execution_start": 1633233729907,
    "execution_millis": 7006,
    "allow_embed": "code_output",
    "deepnote_cell_type": "code"
   },
   "source": "\"\"\"\nCreated on Mon Mar 29 14:13:12 2021\n\n@author: Cao Tri DO\n\"\"\"\n\nimport nltk\n\nimport networkx \nfrom networkx.algorithms.components.connected import connected_components\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\nimport stanza\nstanza.download('en')\nimport collections\n\n\n#%% subfunctions\n\ndef theme_extraction(txt, stop_words, nlp, threshold = 7):\n    \"\"\"\n    Function to extract theme from a verbatims\n\n    Parameters\n    ----------\n    txt : str\n        verbatim to analyze.\n    stop_words : list\n        list of stop to not considered when extract subject.\n    nlp : stanza object\n        stanza object engine.\n    threshold : int, optional\n        DESCRIPTION. The default is 7.\n\n    Returns\n    -------\n    lst_subject : list\n        list of subject analyzed.\n\n    \"\"\"\n    try:\n        # Lower the text\n        doc = nlp(txt.lower())\n        # Initiate void list\n        lst_subject = []\n        \n        # Loop for all sentences in the verbatims\n        for sent in doc.sentences:\n            # If the sentence if too short, keep all in one subject\n            if len(sent._tokens) <= threshold:          \n                # print(\"Sentence is too short. Keep all words\")\n                finalcluster = [word.text for word in sent.words]\n                lst_subject.append(finalcluster)  \n            # For the sentences that are long enough\n            else:\n                # Extract subject using the POS and xparel method\n                # print(\"Sentence is long. Apply subject extraction\")\n                finalcluster = theme_clustering(sent.text, stop_words, nlp)\n                \n                # Only apply the subject engine if there are extracted subject\n                if len(finalcluster)>0: # case when there are issues e.g., 'PRPBLEME D IMPRESSION?????????????????????????????????'\n                    ##################################################################\n                    # Group first element if there are many of them\n                    ##################################################################\n                    # list of size of element\n                    lst_size = [len(element) for element in finalcluster]\n                    # first element of size 1\n                    try:\n                        first_element = next(x[0] for x in enumerate(lst_size) if x[1] > 1)\n                    except: # if all are equal to 1\n                        first_element = len(lst_size)-1\n                    \n                    finalcluster_first_group = []\n                    tmp_first_element = []\n                    for idx_element in range(0,first_element+1):\n                        element = finalcluster[idx_element]\n                        tmp_first_element.extend(element)\n                    finalcluster_first_group.append(tmp_first_element)\n                    for idx_element in range(first_element+1, len(finalcluster)):\n                        element = finalcluster[idx_element]\n                        finalcluster_first_group.append(element)\n                    \n                    ##################################################################\n                    \n                    finalcluster_clean = []\n                    count = 0\n                    tmp = []\n                    for element_idx in range(0,len(finalcluster_first_group)):\n                        element = finalcluster_first_group[element_idx]\n                        if element_idx == 0 and len(element)== 1:\n                            tmp = element\n                        elif len(tmp)>0 or len(element)>1:\n                            tmp.extend(element)\n                            finalcluster_clean.append(tmp)\n                            tmp = []\n                            count = len(finalcluster_clean)-1\n                        else:\n                            tmp.extend(element)\n                            finalcluster_clean[count].extend(tmp)\n                            tmp = []\n                    \n                    for element in finalcluster_clean:\n                        lst_subject.append(element)    \n    except:\n        print(txt)\n        lst_subject = []\n    \n    return lst_subject\n\n\n#%% utilities functions\n\ndef get_meta_data_nlp(txt, nlp):\n    \"\"\"\n    Function to transform stanza object result to dataframe\n\n    Parameters\n    ----------\n    txt : str\n        verbatim to analyze.\n    nlp : stanza object\n        stanza object engine.\n\n    Returns\n    -------\n    df : dataframe\n        dataframe allowing to analyze the results from Stanza.\n\n    \"\"\"\n    doc = nlp(txt)\n    \n    df = pd.DataFrame()\n    count = 1\n    for sentence in doc.sentences:\n        print(sentence._text)\n        for word in sentence.words:\n            df.loc[count, \"word\"]  = word.text\n            df.loc[count, \"lemma\"] = word.lemma\n            # df.loc[count, \"feats\"] = word.feats \n            df.loc[count, \"pos\"] = word.pos\n            df.loc[count, \"upos\"] = word.upos\n            df.loc[count, \"xpos\"] = word.xpos\n            df.loc[count, \"id head\"] = word.head\n            df.loc[count, \"word root\"] = sentence.words[word.head-1].text if word.head > 0 else \"root\"\n            df.loc[count, \"deprel\"] = word.deprel\n            # print(word.text, \" | \", word.lemma, \" | \", word.pos)\n            count = count +1\n    print(df)\n    \n    return df\n\n\ndef get_monolabel(X_test):\n    \"\"\"\n    Extract monolabel from dataframe\n\n    Parameters\n    ----------\n    X_test : dataframe\n        containing the data.\n\n    Returns\n    -------\n    df_monolabel : dataframe\n        Contains only the monolabel.\n    df_multilabel : dataframe\n        Contains only the multilabel.\n    \"\"\"\n    df_theme = X_test.copy()\n    mask = df_theme[\"id_unique\"].value_counts()\n    lst_unique_idx = list(mask[mask>1].index)\n    df_monolabel = X_test[~X_test['id_unique'].isin(lst_unique_idx)].copy()\n    df_multilabel = X_test[X_test['id_unique'].isin(lst_unique_idx)].copy()\n    return df_monolabel, df_multilabel\n\n\n#%% subfunctions for the code \n\ndef theme_clustering(txt, stop_words, nlp):\n    \"\"\"\n    Main program to cluster the subject\n\n    Parameters\n    ----------\n    txt : str\n        verbatim to analyze.\n    stop_words : list\n        list of stop to not considered when extract subject.\n    nlp : stanza object\n        stanza object engine.\n\n    Returns\n    -------\n    lst_merged_cluster_word_all : list\n        list of subject identified by the algorithm.\n\n    \"\"\"\n    # sentList = clean_txt(txt)\n    sentList = clean_txt(txt, nlp)\n\n\n    # for line in sentList:\n    lst_merged_cluster_word_all = []\n    for line in sentList.sentences:\n        \n        # NLTK POS Tagging\n        # taggedList = nltk_pos_tag(line)\n        # taggedList = stanza_pos_tag(line)\n\n        # join consecutive noun\n        # finaltxt, newwordList, taggedList_new, bool_valid_txt = join_consecutive_noun(taggedList, nlp, noun_tag = \"NOUN\")\n        bool_valid_txt = True\n        \n        if bool_valid_txt:\n            # Create Stanford object model\n            # doc = nlp(txt) # Object of Stanford NLP Pipeleine\n            taggedList_new, taggedList_id_new = stanza_pos_tag(line)\n            \n            # Getting the dependency relations betwwen the words\n            # Coverting it into appropriate format\n            dep_node, dep_node_id = get_dependencies(sentList)\n    \n            # we will select only those sublists from the <dep_node> that could probably contain the features\n            totalfeatureList, featureList, featureList_id, categories = select_potential_features(taggedList_new, taggedList_id_new)\n            \n            # Now using the <dep_node> list and the <featureList> we will determine \n            # to which of the words these features in the feature list are related to.\n            fcluster = create_relation(featureList, dep_node)\n            fcluster_id = create_relation(featureList_id, dep_node_id)\n            \n            # finalcluster_out, finalcluster, dic = select_NN_feature_relation(totalfeatureList, fcluster)\n            \n            # Merge list commonelements\n            lst_merged_cluster_id = merge_list_common_elements(fcluster_id)\n            \n            # Order the id within the list\n            lst_merged_cluster_id_sort1 = sort_list_list(lst_merged_cluster_id)\n            \n            # Order the element by id of 1st element\n            lst_merged_cluster_id_keysort = sort_list_by_key(lst_merged_cluster_id_sort1)\n            \n            # Bug fix : if a list is only of 1 element and is contains within the range of the previous list, put it into it\n            # Do not take into account 1st element 0\n            lst_merged_cluster_id_no_orphelin = combine_orphelin_list(lst_merged_cluster_id_keysort)\n            \n            \n            # lst_merged_cluster_id_sorted = sort_list_list(lst_merged_cluster_id)\n            lst_merged_cluster_id_sorted = sort_list_list(lst_merged_cluster_id_no_orphelin)\n            \n            lst_merged_cluster_word = convert_idx_lst_to_word_lst(lst_merged_cluster_id_sorted, taggedList_new)\n            \n            \n        else:\n            lst_merged_cluster_word = []   \n        \n        for element in lst_merged_cluster_word:\n            lst_merged_cluster_word_all.append(element)\n    \n    return lst_merged_cluster_word_all\n\n\ndef combine_orphelin_list(lst_merged_cluster_id_keysort):\n    \"\"\"\n    Function to combine orphelin list if there are within previous range\n    For example : [[1,3], [2], [4]] will give [[1,2, 3], [4]]\n\n    Parameters\n    ----------\n    lst_merged_cluster_id_keysort : list\n        original list to combine.\n\n    Returns\n    -------\n    lst_merged_cluster_id_no_orphelin : list\n        combined list.\n\n    \"\"\"\n    \n    #♦ Assignaton\n    lst_merged_cluster_id_no_orphelin = []\n    \n    # Make a copy\n    lst_merged_cluster_id_no_orphelin.append(lst_merged_cluster_id_keysort[0].copy())\n    id_to_compare = 0\n    \n    # Loop\n    for i in range(1,len(lst_merged_cluster_id_keysort)):\n        # Only if the list is with 1 element\n        if len(lst_merged_cluster_id_keysort[i])==1:\n            id_element = lst_merged_cluster_id_keysort[i][0]\n            # Meta data of previous list\n            previous_list = lst_merged_cluster_id_no_orphelin[id_to_compare]\n            min_id_previous_list = min(previous_list)\n            max_id_previous_list = max(previous_list)\n            # If new list within the range of previous list, merge with it\n            if id_element>=min_id_previous_list and id_element<=max_id_previous_list:\n                lst_merged_cluster_id_no_orphelin[id_to_compare].extend(lst_merged_cluster_id_keysort[i].copy())\n            else:\n                lst_merged_cluster_id_no_orphelin.append(lst_merged_cluster_id_keysort[i].copy())\n                id_to_compare = len(lst_merged_cluster_id_no_orphelin)-1\n        else:\n            lst_merged_cluster_id_no_orphelin.append(lst_merged_cluster_id_keysort[i].copy())\n            id_to_compare = len(lst_merged_cluster_id_no_orphelin)-1   \n    return lst_merged_cluster_id_no_orphelin\n\ndef sort_list_by_key(lst_merged_cluster_id_sort1):\n    \"\"\"\n    Function to sort a list of list into an ordered list\n    For example : [[2], [1,3], [4]] will give [[1,3], [2], [4]]\n\n    Parameters\n    ----------\n    lst_merged_cluster_id_sort1 : list\n        original list unsorted.\n\n    Returns\n    -------\n    final_list : list\n        sorted list.\n\n    \"\"\"\n    dct_list = {}\n    for i in range(0, len(lst_merged_cluster_id_sort1)):\n        first_id = lst_merged_cluster_id_sort1[i][0]\n        dct_list[first_id] = lst_merged_cluster_id_sort1[i]\n    \n    # sort dict key\n    od = collections.OrderedDict(sorted(dct_list.items()))\n    \n    final_list = []\n    for element in od:\n        final_list.append(od[element])\n    return final_list\n    \n\ndef sort_list_list(lst_merged_cluster_id_no_orphelin):\n    \"\"\"\n    Function to sort the list of list to order the id of the words\n\n    Parameters\n    ----------\n    lst_merged_cluster_id : list\n        DESCRIPTION.\n\n    Returns\n    -------\n    lst_merged_cluster_id_sorted : list\n        DESCRIPTION.\n\n    \"\"\"\n    lst_merged_cluster_id_sorted = lst_merged_cluster_id_no_orphelin.copy()\n    for i in range(0, len(lst_merged_cluster_id_sorted)):\n        lst_merged_cluster_id_sorted[i] = sorted(lst_merged_cluster_id_no_orphelin[i]).copy()\n    # Initiate the void dictionnary\n    # dct_idx = {}\n    # # Loop on all element of the list\n    # for element in lst_merged_cluster_id_no_orphelin:\n    #     count = element[0]\n    #     dct_idx[count] = element\n    # # sort the list\n    # dct_idx_sorted = dict(sorted(dct_idx.items(), key=lambda item: item[1]))\n    # lst_merged_cluster_id_sorted = []\n    # for key_element in dct_idx_sorted:\n    #     lst_merged_cluster_id_sorted.append(dct_idx_sorted[key_element])\n\n    return lst_merged_cluster_id_sorted\n\ndef convert_idx_lst_to_word_lst(lst_merged_cluster_id, taggedList_new):\n    \"\"\"\n    Function to convert the idx to the words\n\n    Parameters\n    ----------\n    lst_merged_cluster_id : list\n        list of index.\n    taggedList_new : str\n        list of words.\n\n    Returns\n    -------\n    lst_merged_cluster_word : list\n        list of words.\n\n    \"\"\"\n    lst_merged_cluster_word = []\n    for lst in lst_merged_cluster_id:\n        lst_word = []\n        for element in lst:\n            word_tmp = taggedList_new[element-1][0]\n            lst_word.append(word_tmp)\n        lst_merged_cluster_word.append(lst_word)\n    return lst_merged_cluster_word\n\ndef clean_txt(txt, nlp):\n    \"\"\"\n    Function to clean the text by lower and separate the sentence using stanza\n\n    Parameters\n    ----------\n    txt : str\n        verbatim to analyze.\n    nlp : stanza object\n        stanza object engine.\n\n    Returns\n    -------\n    sentList : list\n        list of sentences.\n\n    \"\"\"\n    txt = txt.lower() # LowerCasing the given Text\n    sentList = nlp(txt)\n    \n    return sentList\n\ndef clean_txt_nltk(txt):\n    \"\"\"\n    Function to clean the text by lower and separate the sentence using nltk\n\n    Parameters\n    ----------\n    txt : str\n        verbatim to analyze.\n    nlp : stanza object\n        stanza object engine.\n\n    Returns\n    -------\n    sentList : list\n        list of sentences.\n\n    \"\"\"\n    txt = txt.lower() # LowerCasing the given Text\n    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n\n    return sentList\n\n\ndef nltk_pos_tag(line):\n    \"\"\"\n    for each sentence in the <sentList> tokenize it and perform POS Tagging and store it into a Tagged List\n    \"\"\"\n    txt_list = nltk.word_tokenize(line) # Splitting up into words\n    taggedList = nltk.pos_tag(txt_list) # Doing Part-of-Speech Tagging to each word\n    return taggedList\n\ndef stanza_pos_tag(line):\n    taggedList = []\n    taggedList_id = []\n    for word in line.words:\n        taggedList.append((word.text, word.pos))\n        taggedList_id.append((word.id, word.pos))\n        \n    return taggedList, taggedList_id\n\ndef join_consecutive_noun(taggedList, nlp, noun_tag = \"NOUN\"):\n    \"\"\"\n    there are many instances where a feature is represented by multiple words \n    so we need to handle that first by joining multiple words features into a one-word feature\n    \n     noun_tag = \"NOUN\" (stanza) or NN (nltk)\n    \"\"\"\n    newwordList = []\n    flag = 0\n    for i in range(0,len(taggedList)-1):\n        if(taggedList[i][1]==noun_tag and taggedList[i+1][1]==noun_tag): # If two consecutive words are Nouns then they are joined together\n            newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n            flag=1\n        else:\n            if(flag==1):\n                flag=0\n                continue\n            newwordList.append(taggedList[i][0])\n            if(i==len(taggedList)-2):\n                newwordList.append(taggedList[i+1][0])\n\n    finaltxt = ' '.join(word for word in newwordList) \n    \n    # Tokenize and POS Tag the new sentence.\n    # new_txt_list = nltk.word_tokenize(finaltxt)\n    # wordsList = [w for w in new_txt_list if not w in stop_words]\n    # taggedList = nltk.pos_tag(wordsList)\n    # print(wordsList)\n    doc = nlp(finaltxt)\n\n    if len(finaltxt) > 0:\n        taggedList, taggedList_id = stanza_pos_tag(doc.sentences[0])\n        bool_valid_txt = True\n    else:\n        taggedList = []\n        bool_valid_txt = False\n\n    return finaltxt, newwordList, taggedList, bool_valid_txt\n\ndef get_dependencies(doc):\n    \"\"\"\n    Function to extract the dependencies from the analysis from Stanza\n\n    Parameters\n    ----------\n    doc : stanza object\n        stanza object applied to a verbatim.\n\n    Returns\n    -------\n    dep_node : list\n        list to tuple of relationship with the words.\n    dep_node_id : list\n        list to tuple of relationship with the ids.\n\n    \"\"\"\n    # Getting the dependency relations betwwen the words\n    dep_node = []\n    dep_node_id = []\n    for dep_edge in doc.sentences[0].dependencies:\n        dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n        dep_node_id.append([dep_edge[2].id, dep_edge[0].id, dep_edge[1]])\n        \n    # Coverting it into appropriate format\n    for i in range(0, len(dep_node)):\n        if (int(dep_node[i][1]) != 0):\n            dep_node[i][1] = dep_node[(int(dep_node[i][1]) - 1)][0]\n            \n    return dep_node, dep_node_id\n\ndef select_potential_features(taggedList_new, taggedList_id_new):\n    \"\"\"\n    Function to extract potential subject from the dep node\n\n    Parameters\n    ----------\n    taggedList_new : TYPE\n        list of words.\n    taggedList_id_new : TYPE\n        list of id words\n\n    Returns\n    -------\n    totalfeatureList : TYPE\n        DESCRIPTION.\n    featureList : TYPE\n        DESCRIPTION.\n    featureList_id : TYPE\n        DESCRIPTION.\n    categories : TYPE\n        DESCRIPTION.\n\n    \"\"\"\n    totalfeatureList = []\n    featureList_id = []\n    featureList = []\n    categories = []\n    count = 1\n    \n    for idx in range(0,len(taggedList_new)):\n        i = taggedList_new[idx]\n        i_idx = taggedList_id_new[idx]\n        # if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n        if(i[1]!='PUNCT'):\n        # if(i[1]=='ADJ' or i[1]=='NOUN' or i[1]=='ADV' or i[1]==\"VERB\" or i[1]==\"PRON\" or i[1]==\"DET\"):\n            featureList.append(list(i)) # For features for each sentence\n            totalfeatureList.append(i) # Stores the features of all the sentences in the text\n            featureList_id.append(list(i_idx))\n            categories.append(i[0])\n        count = count+1\n\n    return totalfeatureList, featureList, featureList_id, categories\n\ndef create_relation(featureList, dep_node):\n    \"\"\"\n    Function to create relationship from the dep_node\n\n    Parameters\n    ----------\n    featureList : list\n        list of potential subjects.\n    dep_node : list\n        tree of dependencies.\n\n    Returns\n    -------\n    fcluster : list\n        list of clusters.\n\n    \"\"\"\n    \n    lst_deprel = [\"nsubj\", \n                  \"acl:relcl\", \n                  \"obj\", \"dobj\", \n                  \"agent\", \n                  \"advmod\", \n                  \"amod\", \n                  \"neg\", \n                  \"prep_of\", \n                  \"acomp\", \n                  \"xcomp\", \n                  \"compound\",\n                  \n                  \"cop\"\n                  ]\n    \n    fcluster = []\n    \n    for i in featureList:\n        filist = []\n        for j in dep_node:\n            if((j[0]==i[0] or j[1]==i[0]) and (j[2] in lst_deprel)):\n                if(j[0]==i[0]):\n                    filist.append(j[1])\n                else:\n                    filist.append(j[0])\n        fcluster.append([i[0], filist])\n    \n    return fcluster\n\ndef select_NN_feature_relation(totalfeatureList, fcluster):\n    \"\"\"\n    Focus the subject on the noun\n\n    Parameters\n    ----------\n    totalfeatureList : list\n        list of potential features for subject.\n    fcluster : list\n        list of extracted potential subjects.\n\n    Returns\n    -------\n    finalcluster_out : list\n        list of extracted potential subjects focus on noun.\n    finalcluster : list\n        list of extracted potential subjects focus on noun.\n    dic : TYPE\n        dic of extracted potential subjects focus on noun..\n\n    \"\"\"\n    finalcluster = []\n    dic = {}\n    \n    for i in totalfeatureList:\n        dic[i[0]] = i[1]\n    \n    for i in fcluster:\n        if(dic[i[0]]==\"NOUN\"):\n            finalcluster.append(i)\n    \n    finalcluster_out = flat_list(finalcluster)\n    \n    return finalcluster_out, finalcluster, dic\n\n\n#%%\n\ndef merge_list_common_elements(fcluster_id):\n    \"\"\"\n    Function to merge list that have common \n\n    Parameters\n    ----------\n    fcluster_id : TYPE\n        DESCRIPTION.\n\n    Returns\n    -------\n    lst_merged_sorted : TYPE\n        DESCRIPTION.\n\n    \"\"\"\n    lst_flat = flat_list(fcluster_id)\n    G = to_graph(lst_flat)\n    set_merged = list(connected_components(G))\n    lst_merged = list(map(list,set_merged))\n    \n    lst_merged_sorted = []\n    for element in lst_merged:\n        lst_merged_sorted.append(sorted(element))\n    \n    return lst_merged_sorted\n\n\ndef flat_list(finalcluster):\n    lst_final = []\n    for lst_tmp in finalcluster:\n        lst_tmp_final = []\n        for element in lst_tmp:\n            if type(element) == list:\n                lst_tmp_final.extend(element)\n            else:\n                lst_tmp_final.append(element)\n        lst_tmp_final = sorted(lst_tmp_final)\n        lst_final.append(lst_tmp_final)\n    \n    # Remove duplicated list\n    set_theme = set(map(tuple,lst_final))\n    lst_theme = list(map(list,set_theme))\n    \n    return lst_theme\n\n\ndef to_graph(l):\n    G = networkx.Graph()\n    for part in l:\n        # each sublist is a bunch of nodes\n        G.add_nodes_from(part)\n        # it also imlies a number of edges:\n        G.add_edges_from(to_edges(part))\n    return G\n\ndef to_edges(l):\n    \"\"\" \n        treat `l` as a Graph and returns it's edges \n        to_edges(['a','b','c','d']) -> [(a,b), (b,c),(c,d)]\n    \"\"\"\n    it = iter(l)\n    last = next(it)\n\n    for current in it:\n        yield last, current\n        last = current    \n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Here is below a code to initiate the `stanza` object that will be used after in our code",
   "metadata": {
    "tags": [],
    "cell_id": "00033-542dc9a0-e175-4c95-8399-a307eeea3c9c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-00eb9c4f-9d21-4ed0-bc32-185f14a04da6",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1e5e942f",
    "execution_start": 1633233393899,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "import stanza\n\ndef initialize_stanza():\n    \"\"\"\n    Function to initialize the stanza engine (default parameters for Engie)\n\n    Returns\n    -------\n    nlp : object\n        stanza object.\n\n    \"\"\"\n    nlp = stanza.Pipeline(lang=\"en\", \n                          use_gpu = True, \n                          verbose = False, \n                          processors= {\n                                          'tokenize':'default', \n                                          'pos' :'default',\n                                          'lemma':'default', \n                                          'depparse':'default',\n                                        })\n    return nlp",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.2 Application on our simple example",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00034-f701a331-3c4c-4dfa-bae3-e7f82a5c52a1",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Let's define a simple example",
   "metadata": {
    "tags": [],
    "cell_id": "00036-66ebb647-d851-44d2-a4ba-904fa35211a4",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00008-c37affd0-0d0b-47ed-abc4-1c4c0508afcb",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "615669e4",
    "execution_start": 1633233397131,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "# Define the text example\ntxt = \"this film is great but the movie was awful. The theatre was amazing\"",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "We first initiate the `stanza` object",
   "metadata": {
    "tags": [],
    "cell_id": "00038-d3bf1a8a-f70d-4663-ad96-9f41056b29b5",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-2042a021-f222-46ee-9b1a-4d3cd60fe986",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9d773ecb",
    "execution_start": 1633233400544,
    "execution_millis": 2326,
    "deepnote_cell_type": "code"
   },
   "source": "# Define the stanza object\nnlp = initialize_stanza()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "We then separate the different phrases in our example",
   "metadata": {
    "tags": [],
    "cell_id": "00040-74e9ee97-d103-4c7d-972c-d0f0fb6a9db1",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00003-509adedb-bfb4-4e2e-92d2-67a679c26d52",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cee606ca",
    "execution_start": 1633233405521,
    "execution_millis": 981,
    "deepnote_cell_type": "code"
   },
   "source": "# Use our custom phrase extraction code\nresult = theme_extraction(txt = txt, stop_words = [], nlp = nlp, threshold = 7)\n\nfor tmp_text in result:\n    print(tmp_text)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "['this', 'film', 'is', 'great', 'but', 'the']\n['movie', 'was', 'awful']\n['the', 'theatre', 'was', 'amazing']\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "We can see from below result that our new package has succeeded in separating the different phrases.\nWe define below the stopword list. Be careful, in our case study, the negation `NOT` is very important and should not be in the stopword list like by default",
   "metadata": {
    "tags": [],
    "cell_id": "00042-cf860177-fd2c-4b00-a1f6-099ee7a6ac0c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00011-e44d37ef-30b7-4c02-871a-e2668f14e99f",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "4ed70ef",
    "execution_start": 1632502646015,
    "execution_millis": 11,
    "deepnote_cell_type": "code"
   },
   "source": "from nltk.corpus import stopwords\n\nnltk.download('stopwords')\nstopword = list(stopwords.words('english'))\nstopword.remove('not')",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "We finally apply the RAKE algorithm",
   "metadata": {
    "tags": [],
    "cell_id": "00044-4745d2e5-1f64-4c71-a9e5-598e9543be01",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00010-66a477b7-9c4b-4169-908f-c1eb8cafa25d",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "7593eac7",
    "execution_start": 1632502648378,
    "execution_millis": 13,
    "deepnote_cell_type": "code"
   },
   "source": "from rake_nltk import Rake\nfrom nltk.corpus import stopwords \nr = Rake(stopwords =stopword) # Uses stopwords for english from NLTK, and all puntuation characters.Please note that \"hello\" is not included in the list of stopwords.\n\nfor tmp_text in result:\n    text= ' '.join(tmp_text)\n    print('-----------------------------')\n    print(text)\n    a=r.extract_keywords_from_text(text)\n    b=r.get_ranked_phrases()\n    c=r.get_ranked_phrases_with_scores()\n    print(a)\n    print(b)\n    print(c)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "-----------------------------\nthis film is great but the\nNone\n['great', 'film']\n[(1.0, 'great'), (1.0, 'film')]\n-----------------------------\nmovie was awful\nNone\n['movie', 'awful']\n[(1.0, 'movie'), (1.0, 'awful')]\n-----------------------------\nthe theatre was amazing\nNone\n['theatre', 'amazing']\n[(1.0, 'theatre'), (1.0, 'amazing')]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "We can observe that the obtained result is what we were expecting",
   "metadata": {
    "tags": [],
    "cell_id": "00046-6ac8cdb3-bd6f-4f2b-b893-63943878d0fc",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 3.3 Conclusion",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00040-51bb6823-b7cb-4d06-bbb0-15f6cab6cc1d",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "From this simple example, we show that we achieved our objective of getting the subject by assemble 2 methodologies:\n- phrase separation\n- subject extraction",
   "metadata": {
    "tags": [],
    "cell_id": "00040-dd8fadd6-06c1-470f-ad32-bc808afe9768",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<center><img src=\"https://media.makeameme.org/created/victory-at-last.jpg\" title=\"Python Logo\" width = 300/></center>\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00049-3fa0d8e4-4c46-4632-9843-28ac6130a190",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "tags": [],
    "cell_id": "00042-de810043-d6f3-4925-9b40-81d5bf4604ec",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=22d1188b-dbee-4618-bda5-79d4ace33c29' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "47e667f1-b4f0-4a5c-9057-0495b270894e",
  "deepnote_execution_queue": []
 }
}